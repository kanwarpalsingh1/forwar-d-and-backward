{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33caf8df-c59e-48f0-a294-d57cdb2a2dd5",
   "metadata": {},
   "source": [
    "\n",
    "Q1. The purpose of forward propagation in a neural network is to compute the output of the network given a set of input features. It involves passing the input data through the network's layers, applying weights and biases, and activating neurons using activation functions to produce the final output.\n",
    "\n",
    "Q2. In a single-layer feedforward neural network, forward propagation is implemented mathematically by performing a series of dot products between the input features and the weights, adding biases, and applying an activation function to produce the output. Mathematically, the output \n",
    "�\n",
    "^\n",
    "y\n",
    "^\n",
    "​\n",
    "  is calculated as:\n",
    "\n",
    "�\n",
    "^\n",
    "=\n",
    "�\n",
    "(\n",
    "�\n",
    "⋅\n",
    "�\n",
    "+\n",
    "�\n",
    ")\n",
    "y\n",
    "^\n",
    "​\n",
    " =f(W⋅X+b)\n",
    "\n",
    "Where:\n",
    "\n",
    "�\n",
    "^\n",
    "y\n",
    "^\n",
    "​\n",
    "  is the predicted output,\n",
    "�\n",
    "X is the input features,\n",
    "�\n",
    "W is the weight matrix,\n",
    "�\n",
    "b is the bias vector, and\n",
    "�\n",
    "(\n",
    ")\n",
    "f() is the activation function.\n",
    "Q3. Activation functions are used during forward propagation to introduce non-linearity into the network, allowing it to learn complex patterns in the data. These functions determine the output of each neuron based on the weighted sum of its inputs. Common activation functions include sigmoid, tanh, ReLU, and softmax.\n",
    "\n",
    "Q4. In forward propagation, weights and biases determine the strength of connections between neurons and the neuron's output, respectively. The weights are multiplied with the input features, and biases are added to the weighted sum before passing through the activation function. These parameters are learned during the training process to minimize the difference between the predicted and actual outputs.\n",
    "\n",
    "Q5. The softmax function is typically applied in the output layer during forward propagation when the neural network is used for multi-class classification tasks. It converts the raw output scores of the network into probabilities, where each output represents the probability of belonging to a particular class. This allows the network to output a probability distribution over all classes.\n",
    "\n",
    "Q6. The purpose of backward propagation, also known as backpropagation, in a neural network is to calculate the gradient of the loss function with respect to the model parameters (weights and biases). It allows the network to update its parameters by moving in the direction that minimizes the loss, thus improving its performance.\n",
    "\n",
    "Q7. In a single-layer feedforward neural network, backward propagation is mathematically calculated using the chain rule of calculus to compute the gradient of the loss function with respect to each parameter. The gradients are then used to update the parameters using an optimization algorithm such as gradient descent. Mathematically, the gradients are computed as:\n",
    "\n",
    "∂\n",
    "�\n",
    "∂\n",
    "�\n",
    "=\n",
    "�\n",
    "�\n",
    "⋅\n",
    "∂\n",
    "�\n",
    "∂\n",
    "�\n",
    "^\n",
    "∂W\n",
    "∂L\n",
    "​\n",
    " =X \n",
    "T\n",
    " ⋅ \n",
    "∂ \n",
    "y\n",
    "^\n",
    "​\n",
    " \n",
    "∂L\n",
    "​\n",
    " \n",
    "∂\n",
    "�\n",
    "∂\n",
    "�\n",
    "=\n",
    "∂\n",
    "�\n",
    "∂\n",
    "�\n",
    "^\n",
    "∂b\n",
    "∂L\n",
    "​\n",
    " = \n",
    "∂ \n",
    "y\n",
    "^\n",
    "​\n",
    " \n",
    "∂L\n",
    "​\n",
    " \n",
    "\n",
    "Where:\n",
    "\n",
    "�\n",
    "L is the loss function,\n",
    "∂\n",
    "�\n",
    "∂\n",
    "�\n",
    "∂W\n",
    "∂L\n",
    "​\n",
    "  is the gradient of the loss function with respect to the weights,\n",
    "∂\n",
    "�\n",
    "∂\n",
    "�\n",
    "∂b\n",
    "∂L\n",
    "​\n",
    "  is the gradient of the loss function with respect to the biases,\n",
    "�\n",
    "�\n",
    "X \n",
    "T\n",
    "  is the transpose of the input features, and\n",
    "∂\n",
    "�\n",
    "∂\n",
    "�\n",
    "^\n",
    "∂ \n",
    "y\n",
    "^\n",
    "​\n",
    " \n",
    "∂L\n",
    "​\n",
    "  is the gradient of the loss function with respect to the predicted output.\n",
    "Q8. The chain rule is a fundamental concept in calculus that allows us to compute the derivative of a composite function. In the context of backward propagation, the chain rule is applied to calculate the gradient of the loss function with respect to each parameter by recursively applying the derivatives of intermediate functions. It decomposes the gradient computation into a series of smaller, simpler steps, making it computationally efficient.\n",
    "\n",
    "Q9. Some common challenges or issues that can occur during backward propagation include vanishing or exploding gradients, which can hinder the training process and lead to slow convergence or unstable training. These issues can be addressed by using appropriate weight initialization techniques, gradient clipping, and normalization methods such as batch normalization. Additionally, choosing an appropriate learning rate and optimization algorithm can help mitigate these challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715e1820-ddb4-4295-9dee-ccb04317d398",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
